# Build docker Image to run httpServer
# with the docs and forms contents 
# pre-copied into the docker image.  This 
# version of DockerFile was tested on 
# a Gigbyte mini-pc running Intel clear 
# Linux.
#
# cd into mforms checkout directory
sudo docker build --no-cache .
# Should show build sucessful with an ID we will use in next step

# Build the docker image with a tag mforms.
sudo docker build --tag=mforms .

# Build image with tag of mforms and memory limit of 300meg.
sudo docker build --tag=mforms -m=300m .

# Build image with tag of mforms and memory 
# limit of 300meg with cpu limited to 30%
# of one core.  Default setting for CPU is 
# -c=1024 so by setting this to 300 will allow
# this process to consume about 1/3 of normal 
# when other processes need the CPU.
# https://docs.docker.com/engine/reference/run/
sudo docker build --tag=mforms -m=300m -c=300 .


# List lattest docker image build with tag mforms
# https://www.grymoire.com/Unix/Sed.html
sudo docker image ls -a | grep mfroms.*latest

# Extract the image ID for the most recent image 
# that matches the tag. Save it in a variable 
# and use that variable to run the image.
imgId=$(sudo docker image ls -a | grep mforms.*latest | sed -r 's/[ ]+/ /g' | cut -d ' ' -f 3)
echo $imgId
sudo docker run --rm  -p 9831:9831 $imgId
sudo docker save -o mforms.docker.tgz $imgId

# Save a docker image to a file that could be
# copied to another server.
sudo docker save -o t.t 02f9487b14cc 

# Produce docker image and save to a file
# https://docs.docker.com/engine/reference/commandline/build/
docker build --output type=tar,dest=out.tar .

# Produce a docker image and save
# to repository.
$ docker build -t myname/myapp --build-arg BUILDKIT_INLINE_CACHE=1 .
$ docker push myname/myapp
# Then build a new image using the repositories
# from the published image.
docker build --cache-from myname/myapp .


# Run the docker image replaces the 47d... with image id built locally
sudo docker run  -p 9831:9831 47d3189040ae

# List Most recent started docker images running along with size
# to list more images remove the -l option. 
sudo docker ps -s -l

# Stop the Docker process so we can rebuild and re-launch
# repalces 5ee.... with container ID from ps.
sudo docker stop 5ee03799d9da

# List Local Docker images
sudo docker image ls


# Prune dangeling images
sudo docker image prune

# Prune all images without an active container
# https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes
# https://www.freecodecamp.org/news/docker-image-guide-how-to-remove-and-delete-docker-images-stop-containers-and-remove-all-volumes/
docker system prune -a


# Run a docker container but remove when exited.
docker run --rm image_name

# Remove all exited docker containers
docker rm $(docker ps -a -f status=exited -q)

# Remove and stop all exited Docker containers
docker stop $(docker ps -a -q)
docker rm $(docker ps -a -q)

# Remove Docker Images according to a pattern
docker images -a | grep "pattern" | awk '{print $3}' | xargs docker rmi

# Share a local folder with a container
docker run --name=my_container -d -v $(pwd)/tmp:/var/log/nginx -p 8080:80 nginx

# https://docs.docker.com/registry/spec/api/#listing-repositories


################
### Research 
################
# https://blog.container-solutions.com/when-is-the-wrong-time-to-use-kubernetes So 
the lesson of Step One is: Risk reduction, not cost reduction. Again, looking only 
at hardware costs it is true the cost will typically go up.  It's always better 
to be in a value creation business rather than a cost-saving one.  If you do 
Kubernetes before you’re ready, it is literally worse than not doing it at all.

 

# Example of how minor configuration issue in Kubernetes caused
# widespread outage even on a Google administered cluster.
# https://blog.jetstack.io/blog/gke-webhook-outage/
# https://github.com/kubernetes/kubernetes/issues/54522

# Example Kubernetes in AWS made latency 10X higher 
# through no fault of Kubernetes. 
https://srvaroa.github.io/kubernetes/migration/latency/dns/java/aws/microservices 
/ 2019/10/22/kubernetes-added-a-0-to-my-latency.html 
Problems often appear just 
because we put some pieces together in the first place. Two independently sound 
decisions don’t necessarily make sense together.


# Fun Talk of crashing with Istio and Kubernetes
# Ultimately needed to recreate multiple kubernetes clusters.
# due to weakness in Istio for production mutli-tenant
# deployments.
https://medium.com/@jakubkulich/sailing-with-the-istio-through-the-shallow-water- 
8ae81668381e   bug in a service discovery of Istio, which doesn’t update IP 
addresses of headless services with ServiceEntry.  Consequently, your service 
won’t be able to do so because the proxy sidecar is already shutdown. because 
Istio is the single point of the failure. Istio made some changes that made Istio 
undeployable in the multitenant setup.






